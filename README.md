# Legal-Risk-Audit
Automating Contract Compliance &amp; Litigation Oversight
In this project, I’m combining my legal background with data analytics to bridge the gap between contract language and litigation outcomes. As various data solutions such as Power BI, Tableau, Databricks and Fabric become more ascendant, Microsoft Excel still offers wide-ranging capabilities, accessible and familiar to all kinds of stakeholders. This is important. Using Excel, I will analyse two fictional datasets: one focused on contractual compliance and the other on case management efficiency. All analysis was performed on anonymized, public-domain datasets sourced from Kaggle; no Personally Identifiable Information (PII) was processed or stored during this project.

First, I’ll examine a table of financial services contracts. While high-volume complexity is standard in this industry, it often masks operational and regulatory risks. I will use Excel to build a scalable solution that filters these dense documents for 'red-flag' language and identifies transaction types most exposed to compliance issues. Top rows here (screenshot 1).
Second, I will audit a firm’s case management data—tracking claim amounts, outcomes, and lawyer seniority. My goal is to confirm if the firm is operating efficiently, ensuring settlement rates are optimized and that the most experienced lawyers are assigned to the highest-risk cases. Top rows here (screenshot 2).
The first thing to do is follow Excel’s warning about data loss. To keep the data safe and to make working with the file easier, simply select ‘Save As’ and save the file, preferably in .xlsx format. It can be returned to .csv format after the work is done. (screenshot 3).

Before working with any data, it is a good idea to inspect and profile the data. The immediate issue is that we are missing a column header in column 1, so I changed that to ‘Contract_ID’ to make every row trackable. (Screenshot 4). 	Because the index column should only contain unique values, I used conditional formatting in the home ribbon to check for duplicates (screenshot 5). To my surprise, they were all red (screenshot 6). I used the ctrl+f functionality to inspect why this might be the case, is it supposed to be like this, or do I have simple duplicates? I selected a random number, 5000, and took a look. While the number is the same, the clause_text and clause_type differ. (screenshot 7, 8)

This discovery warranted a change of strategy. I removed the conditional formatting, and inserted a now index column, the row_id (screenshot 9). The basic numerical feature was used to create unique ids for each row (screenshot 10). I then made sure that each column has the correct data type (screenshot 11). It is also important to standardise the text columns, so I used ctrl+shift+L to do that (screenshot 12). There were some immediate issues to address in the clause_type column: capitalisation inconsistency, and varied use of ‘-‘ vs ‘_’ to indicate spaces. (screenshots 13-16).

To fix the capitalisation issue, I right-clicked column E and used the insert function, as before, to create a new column. I used the =LOWER(D2) to fill the new column with the lower-case values of clause_type column. This then uniformed the use of upper vs lower-case (screenshot 17). I then copied this new column, and pasted it into the old column (this is important to avoid a REF error) (screenshot18). Next, I used the ctrl H function to replace ‘-‘ with ‘_’ (screenshot19). This data normalisation makes for more accurate analysis, be it by myself or my colleagues downstream.

In the clause_text column, there are various issues, including what appears to be an encoding error in row 1. I used the same replace function to tidy this up, replacing, for example,  â€œ with “. This action returned a whopping 6792 replacements (screenshot20). And another 12768 when I did the same with â€. This is to ensure that the letter and word count is accurate.

A clause text can not be inspected, if there is no clause text, so I inspected the column for blanks using the filter arrows created earlier. I found 44 blank rows (screenshot21). In an analytical/statistical workload, the rows with a blank clause and a blank word/letter count could be removed. In a compliance workload, they should be isolated and flagged for review. To ensure best of both worlds, I copied all of these rows to a new worksheet (screenshot22) then deleted them from the main table.

(As a quick walk-back, I realised my word and letter count may be slightly affected by my cleaning actions. Encoding errors such as â€œ could take up 3 characters or more. To solve this, I checked whether the counts were formula-based or simply hardcoded. As the latter was true, I turned to Gemini for an Excel function to fix the word count =IF(LEN(TRIM(C2))=0, 0, LEN(TRIM(C2)) - LEN(SUBSTITUTE(TRIM(C2), " ", "")) + 1), and used =LEN(C2) to fix the letter count (screenshot20a)).

With the data cleaned, I wanted to see how much of each clause_type I had, so created a pivot table (home > insert > pivot table), and sorted the values from largest to smallest to find which clause type had the highest number of instances, and by extension where most compliance risk was concentrated (screenshot23).

From the sorted table (screenshot 23b) we can see the most frequently occurring clause types. In order to create a warning label, I generated a list of compliance-risky language. The obvious challenge here is that such clauses will almost never say the quiet part out loud. Absolutely no clause will ever say “this is the bribe we talked about”. First, I used my own legal background to think of some generally risky language, then asked Gemini to expand upon it, making sure not to give more details than strictly necessary for it to complete the task (screenshot 24). The output from AI gave me more words to work with, and I added it to the list. To complete the creation of my threat-engine, I used =RAND() to shuffle the table and pull 20 random rows from the source table for manual validation and further extraction of potentially unsafe language (screenshot 25). I derived more trigger words from these columns. (The AI output and the findings from the 20 rows are not stated because in theory, the more that is known about your compliance screening practises, the easier it is to avoid them). (Frustratingly, more encoding errors were found and resolved: Â§, Â§2, Â§, Â. Long sequences of periods were also replaced with a single period).
To finally implement my threat engine, I made a new sheet and created a table with these columns: key_word, category, weight. 10 represents the highest possible severity or risk-level, 1 represents the lowest risk, and by extension the lowest urgency for review (screenshot 26).

To create the scorer, I used the following formula =SUMPRODUCT(ISNUMBER(SEARCH(threat_engine!$A$2:$A$48, C2)) * threat_engine!$C$2:$C$48) in H2 of the source table, to refer back to my threat_engine sheet values.  Each row was then given a score. Absolute cell-references were used to ensure future usability and scalability of the formula. Column H was given the name risk_score. I then applied conditional formatting to colour code the rows based on how risky they were, then used conditional formatting and colour scales before sorting the table from high-to-low. The result looked like so (screenshot 27). This formatting gives us options: for example, we can filter rows by colour, selecting the red ones for urgent review, or even the green ones, to check that nothing slipped through the cracks (screenshot28).
I had two questions remaining: Do some types of clauses more carry more liability? Is there a relationship between word-count and compliance-risk?
I used a pivot chart to tackle the first question. I dragged clause_type into rows, and changed risk_score from sum, to average. 

The threat engine highlighted three specific types of contract language that could create the most trouble for a business:
•	Assignment: These clauses let a partner pass their side of the deal to a third party. This is risky because you might find yourself indirectly doing business with irreputable individuals or entities.
•	Confidentiality: While privacy is a reasonable expectation, some contracts use overly strict secrecy to hide who is actually behind a transaction, creating the same kind of risks as those mentioned in the assignment clause. It can also obstruct financial auditing
•	Indemnification: This creates great risk for companies. In the worst cases, the firm could find themselves paying massive fines or facing other serious penalties for the mistakes, oversights or wrongdoing of others.

To check word-count vs compliance risk, I created another pivot table and followed similar steps, but used the group-by feature to create bins (screenshot30). For readability I reduced the decimal places and had the following table. (screenshot31). My table shows at the very least, a correlation between clause length and compliance risk: as length increases, so does the score. There is an outlier though. The longest clause by word count has no compliance risk. Upon investigation, it appears to be a n index of terms (with so many periods that my previous sweep missed it). While long, it lacked any intent words, and consisted mostly of periods.

A final issue is that my risk score is scoring clauses per entry. But many contract clauses appear two or more times. While the source table gives us a granular risk score, what about the risk of each contract whole? Once again I used a pivot table, dragging contract_id to the rows category, and sum of risk_score to the values category. I then right clicked the risk_score column and sorted it, highest to lowest (screenshot33). Contract 7886 is listed as the riskiest, with a total score of 37, I checked the accuracy of the score by find its entries in the base table. I found clause 7886 in the base table and as you can see, there are two clauses, one with a score of 35, the other with 2, making the contract´s total risk score 37, so the pivot table was a success. (screenshot34a,b)
After doing some tidying, such as renaming the columns in my pivot tables to more readable names (to clause_type, and word count rage, respectively) I concluded my exploration of the first spreadsheet. It should be noted that a risk_score would not constitute an allegation, but merely a review and if needed, a request for further clarification.

Spreadsheet 2: Case-efficiency
In this spreadsheet (screenshot 2), we have a table of cases, their types, claim amount, and if they have been settled or not. First, I changed the settlement amount to currency using the home ribbon. For our purposes, I will choose Euros. I later added the comma to the values using the same tool ribbon, to make values more human-readable. I inspected the case_type column using ctrl+L, to check for typos, duplicate categories or similar, and there were no issues, we have just 4 categories of case (screenshot 36). I did the same for the settled case, to ensure every entry follows its intended binary format: 0 or 1. All values were true to format.

To orientate myself in this new dataset, I performed some quick summary statistics: 
I used this formula =COUNT(C:C) to get a quick row count (100). 
=SUM(C2:C101) was used to see the total claim amount
Then =AVERAGE(C2:C101) to find the average claim size
Similar formulae were used for lawyer experience, but in order to categorise relative experience of lawyers within a firm, more information is needed. The following were used:
=MAX(B2:B101)
=MIN(B2:B101)
=MEDIAN(B2:B101)
=MODE.SNGL(B2:B101)
I colour-filled my work so as to visually and logically separate them from the main table, should anyone downstream also need to view or use the table (screenshot 37).  I later moved them to a new sheet – summary statistics, so they didn´t get in the way of my pivot tables.

Now I have more info on the experience levels of the firm’s lawyers, I can use conditional formatting and make a decision on how to categorise and colour-code them. The mean, mode and median are all very close, suggesting few outliers. In the end I settled on these parameters (screenshot 38). Anyone above 20 is highly experienced and shaded green, anyone with below the 14 average is shaded in red. Those at or just above the average in yellow.

(In my exuberance, I almost forgot to insert a case_id index column, that was added now.).
(I also noticed my header was made green by conditional formatting. I changed the rule to between 20 and 50, so it would hold up in the long term, and not hade my header. There are admittedly better systemic fixes to this, but I was able to move on quickly this way.).

As some cases are settled and some not, it would be a good idea to see how much money has been won by the firm, and how much is still on the line. For this, I once again turned to pivot tables. I created the table putting settled in rows, and claim_amount in values, making sure it is summed. There was an unexpected blank entry. To investigate, I checked the total amount against my earlier summary statistics, and they matched. I then used ctrl+G > special, to check the settled and claim_amount columns for blanks, and none were returned. With this, I saw it safe to continue, and simply filtered out the blanks row from the pivot table. From the table, we can see that 27 million Euros has been won by the firm, but another 22 million Is on the line. (screenshots 39a,b,c).
To gain actionable insights, I also used pivot tables to check lawyer experience vs both settled and unsettled cases: Could more money have been won? Are the firm maximising their chances in the remaining cases? Are the most experienced lawyers taking the biggest cases?

To see if more money could have been won generally, I created another pivot table and set it up like so, then used the group function (screenshot X39). The results were counter-intuitive. The less-experienced lawyers were responsible for the greatest share of the money in general. When set tocases settled, the difference was less pronounced, but the most experienced lawyers accounted for the lowest amount won (screenshot 40a, b). While there may be different reasons for this, and a slightly less experienced lawyer is by no means a bad lawyer – especially at a large firm, it is also possible that even more money may have been won for clients, if more senior staff handled the biggest cases.

This brings me to the next question: Are the biggest lawyers working on the biggest unsettled cases? Once again, pivot table. I created it as such and found something interesting – junior lawyers are responsible for up to half of the money tied up in unsettled cases – this represents a considerable operational risk.

I wanted to check one last thing – are junior lawyers being allocated a specific type of case? First I checked how many of each case the firm has generally (41) then filtered it as such (42). Here, we can see that junior lawyers are tied up mostly in corporate and criminal cases. There are some clear issues present:
-The junior staff are handling the bulk of the active case work, almost 50%
-Junior staff are handling 67% of the open corporate cases, while mid-level lawyers are only working on one such case

The first actional insight would be to investigate and rectify this as junior staff are handling the majority of corporate files, which are highly complex and often involve tremendous amounts of money. They also require a high level of experience and negotiation leverage that may not be possible for junior lawyers. Should this affect the firm’s ability to win cases, it could have a significant reputational impact. 
Another important step is to address the under-utilisation of mid-tier staff. Not doing so could expose the firm to different risks: failing to maximise the chances of winning more cases, and gaining larger settlement amounts, and the possibility that talented mid-tier staff could feel under-valued, leading to difficulty retaining top legal professionals. The firm should implement a "Tiered Allocation Policy." By moving a portion of the 8 unsettled Corporate cases and the high-value Civil files from the 1–10 year group over to the 11–20 year group, the firm immediately balances its risk profile.

This concludes my analysis of the 2nd spreadsheet, and my last project for 2025. Thanks to the providers of the datasets I used, found here (https://www.kaggle.com/datasets/mohammedalrashidan/contracts-clauses-datasets) and here (https://www.kaggle.com/datasets/maryakinde/legal-case-analytics) and a happy new year to everybody!.
